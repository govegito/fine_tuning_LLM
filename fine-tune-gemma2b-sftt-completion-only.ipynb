{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["pip install -q -U bitsandbytes transformers peft accelerate datasets trl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n","import torch\n","from huggingface_hub import login\n","login(\n","  token=\"\", # ADD YOUR TOKEN HERE\n","  add_to_git_credential=True\n",")\n","import os\n","# disable Weights and Biases\n","os.environ['WANDB_DISABLED']=\"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def formatting_prompts_func(sample):\n","    output_texts = []\n","    for i in range(len(sample['context'])):\n","        INTRO=\"<bos>### You are an expert in writing SQL queries based on schema given\"\n","        INSTRUCTION=\"\\n Given below schema and a query, try to write a SQL script\"\n","        schema_text=\"\".join([part+\"\\n\" for part in sample[\"context\"][i].split(\";\")])\n","        SCHEMA=f\"\\n ###SCHEMA: {schema_text}\" if schema_text else none\n","        QUERY=f\"\\n ###QUERY: {sample['question'][i]}\"\n","        RESPONSE=f\"\\n\\n ###RESPONSE: {sample['answer'][i]}\"\n","        END=\"<eos>\"\n","        formatted_prompt=\"\\n\".join([part for part in [INTRO,INSTRUCTION,SCHEMA,QUERY,RESPONSE,END]])\n","        output_texts.append(formatted_prompt)\n","    return output_texts"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n","dataset = dataset.shuffle().select(range(12500))\n","dataset = dataset.train_test_split(test_size=2500/12500)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from trl import setup_chat_format\n","\n","compute_dtype = getattr(torch, \"float16\")\n","quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=False,\n","    )\n","model_name=\"google/gemma-2b-it\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","device_map = {\"\": 0}\n","model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quantization_config,device_map=device_map)\n","tokenizer.padding_side = 'right' # to prevent warnings"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import LoraConfig\n"," \n","# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n","peft_config = LoraConfig(\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        r=32,\n","        bias=\"none\",\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n","        task_type=\"CAUSAL_LM\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["output_dir = f'./peft-gemma-2b-sql-SFTT'\n","\n","args = TrainingArguments(\n","    output_dir=output_dir,                  # directory to save and repository id\n","    num_train_epochs=1,                     # number of training epochs\n","    per_device_train_batch_size=1,          # batch size per device during training\n","    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n","    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n","    logging_steps=25,                       # log every 10 steps\n","    save_strategy=\"steps\",                  # save checkpoint every epoch\n","    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n","    fp16=True,                              # use bfloat16 precision\n","    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n","    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n","    max_steps=500,\n","    save_steps=100,\n","    overwrite_output_dir = 'True',\n","   \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from trl import SFTTrainer,DataCollatorForCompletionOnlyLM\n"," \n","max_seq_length = 1024 # max sequence length for model and packing of the dataset\n","response_template = \" ###RESPONSE:\"\n","collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=dataset['train'],\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    formatting_func=formatting_prompts_func,\n","    data_collator=collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# start training, the model will be automatically saved to the hub and the output directory\n","trainer.train()\n"," \n","# save model\n","trainer.save_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","from transformers import pipeline\n","peft_model = AutoPeftModelForCausalLM.from_pretrained(\n","  output_dir,\n","  device_map=\"auto\",\n","  torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def formatting_prompt(sample):\n","    INTRO=\"<bos>### You are an expert in writing SQL queries based on schema given\"\n","    INSTRUCTION=\"\\n Given below schema and a query, try to write a SQL script\"\n","    schema_text=\"\".join([part+\"\\n\" for part in sample[\"context\"].split(\";\")])\n","    SCHEMA=f\"\\n ###SCHEMA: {schema_text}\" if schema_text else none\n","    QUERY=f\"\\n ###QUERY: {sample['question']}\"\n","    RESPONSE=f\"\\n\\n ###RESPONSE:\"\n","#     END=\"<eos>\"\n","    formatted_prompt=\"\\n\".join([part for part in [INTRO,INSTRUCTION,SCHEMA,QUERY,RESPONSE]])\n","    return formatted_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quantization_config,device_map=device_map)\n","base_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from random import randint\n","\n","eval_dataset = dataset['test']\n","\n","for i in range(5):\n","    rand_idx = randint(0, len(eval_dataset))\n","\n","    # Test on sample\n","    prompt = formatting_prompt(eval_dataset[rand_idx])\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","    outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","    base_model_outputs = base_pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","\n","    print(f\"Context:\\n{eval_dataset[rand_idx]['context']}\")\n","    print(f\"Query:\\n{eval_dataset[rand_idx]['question']}\")\n","    print(f\"Original Answer:\\n{eval_dataset[rand_idx]['answer']}\")\n","    \n","    print(f\"Generated Answer:\\n{outputs[0]['generated_text'].split('###RESPONSE:')[1]}\")\n","\n","    print(f\"Base model Generated Answer:\\n{base_model_outputs[0]['generated_text'].split('###RESPONSE:')[1]}\")\n","\n","    print(\"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

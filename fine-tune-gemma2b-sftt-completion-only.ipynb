{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q -U bitsandbytes transformers peft accelerate datasets trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\nimport torch\nfrom huggingface_hub import login\nlogin(\n  token=\"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\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)\nimport os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def formatting_prompts_func(sample):\n    output_texts = []\n    for i in range(len(sample['context'])):\n        INTRO=\"<bos>### You are an expert in writing SQL queries based on schema given\"\n        INSTRUCTION=\"\\n Given below schema and a query, try to write a SQL script\"\n        schema_text=\"\".join([part+\"\\n\" for part in sample[\"context\"][i].split(\";\")])\n        SCHEMA=f\"\\n ###SCHEMA: {schema_text}\" if schema_text else none\n        QUERY=f\"\\n ###QUERY: {sample['question'][i]}\"\n        RESPONSE=f\"\\n\\n ###RESPONSE: {sample['answer'][i]}\"\n        END=\"<eos>\"\n        formatted_prompt=\"\\n\".join([part for part in [INTRO,INSTRUCTION,SCHEMA,QUERY,RESPONSE,END]])\n        output_texts.append(formatted_prompt)\n    return output_texts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\ndataset = dataset.train_test_split(test_size=2500/12500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import setup_chat_format\n\ncompute_dtype = getattr(torch, \"float16\")\nquantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\nmodel_name=\"google/gemma-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndevice_map = {\"\": 0}\nmodel = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quantization_config,device_map=device_map)\ntokenizer.padding_side = 'right' # to prevent warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig\n \n# LoRA config based on QLoRA paper & Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=32,\n        lora_dropout=0.05,\n        r=32,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n        task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = f'./peft-gemma-2b-sql-SFTT'\n\nargs = TrainingArguments(\n    output_dir=output_dir,                  # directory to save and repository id\n    num_train_epochs=1,                     # number of training epochs\n    per_device_train_batch_size=1,          # batch size per device during training\n    gradient_accumulation_steps=4,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=25,                       # log every 10 steps\n    save_strategy=\"steps\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    fp16=True,                              # use bfloat16 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    max_steps=500,\n    save_steps=100,\n    overwrite_output_dir = 'True',\n   \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer,DataCollatorForCompletionOnlyLM\n \nmax_seq_length = 1024 # max sequence length for model and packing of the dataset\nresponse_template = \" ###RESPONSE:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset['train'],\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    formatting_func=formatting_prompts_func,\n    data_collator=collator\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n \n# save model\ntrainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\nfrom transformers import pipeline\npeft_model = AutoPeftModelForCausalLM.from_pretrained(\n  output_dir,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\npipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def formatting_prompt(sample):\n    INTRO=\"<bos>### You are an expert in writing SQL queries based on schema given\"\n    INSTRUCTION=\"\\n Given below schema and a query, try to write a SQL script\"\n    schema_text=\"\".join([part+\"\\n\" for part in sample[\"context\"].split(\";\")])\n    SCHEMA=f\"\\n ###SCHEMA: {schema_text}\" if schema_text else none\n    QUERY=f\"\\n ###QUERY: {sample['question']}\"\n    RESPONSE=f\"\\n\\n ###RESPONSE:\"\n#     END=\"<eos>\"\n    formatted_prompt=\"\\n\".join([part for part in [INTRO,INSTRUCTION,SCHEMA,QUERY,RESPONSE]])\n    return formatted_prompt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quantization_config,device_map=device_map)\nbase_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint\n\neval_dataset = dataset['test']\n\nfor i in range(5):\n    rand_idx = randint(0, len(eval_dataset))\n\n    # Test on sample\n    prompt = formatting_prompt(eval_dataset[rand_idx])\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    base_model_outputs = base_pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\n    print(f\"Context:\\n{eval_dataset[rand_idx]['context']}\")\n    print(f\"Query:\\n{eval_dataset[rand_idx]['question']}\")\n    print(f\"Original Answer:\\n{eval_dataset[rand_idx]['answer']}\")\n    \n    print(f\"Generated Answer:\\n{outputs[0]['generated_text'].split('###RESPONSE:')[1]}\")\n\n    print(f\"Base model Generated Answer:\\n{base_model_outputs[0]['generated_text'].split('###RESPONSE:')[1]}\")\n\n    print(\"\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Set up environment\n","Let's start by installing some huggingface libraries which will be needed to do supervised fine tuning of our pretrained LLM model. \n","- transformers library for loading up LLM models.\n","- datasets for downloading datasets from huggingface and preparing them for inferencing and fine tuning.\n","- bitsandbytes for quantization of LLM weights from higher precision format to lower precision.\n","- peft stands for parameter efficient fine tuning which implements LoRA adapters, which lowers down the hardware resources needed for fine tuning.\n","- trl is for using supervised fine tuning trainer class."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["pip install -q -U bitsandbytes transformers peft accelerate datasets trl"]},{"cell_type":"markdown","metadata":{},"source":["## Import all necessary modules and loging into huggingface"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n","import torch\n","from huggingface_hub import login\n","login(\n","  token=\"\", # ADD YOUR TOKEN HERE\n","  add_to_git_credential=True\n",")\n","import os\n","# disable Weights and Biases\n","os.environ['WANDB_DISABLED']=\"true\""]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Format Support for SFTTrainer\n","\n","The `SFTTrainer` supports popular dataset formats, allowing you to pass the dataset to the trainer directly without any pre-processing. The following formats are supported:\n","\n","## Conversational Format\n","\n","```json\n","{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n","{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n","```\n","\n","## Instruction Format\n","\n","```json\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n","{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n","```\n","\n","If your dataset uses one of the above formats, you can directly pass it to the trainer without pre-processing. The `SFTTrainer` will format the dataset for you using the defined format from the model’s tokenizer with the `apply_chat_template` method.\n","\n","Now, we will convert our dataset into conversational format in this step, downsample from the dataset and select 12500 rows and make a train split of 10000 rows."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","{schema}\"\"\"\n"," \n","def create_conversation(sample):\n","  return {\n","    \"messages\": [\n","      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n","      {\"role\": \"user\", \"content\": sample[\"question\"]},\n","      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n","    ]\n","  }\n"," \n","dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n","dataset = dataset.shuffle().select(range(12500))\n"," \n","dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n","dataset = dataset.train_test_split(test_size=2500/12500)\n"," \n","print(dataset[\"train\"][345][\"messages\"])\n"," "]},{"cell_type":"markdown","metadata":{},"source":["# Loading LLM for SFT\n","\n","Next, we will load our LLM and tokenizer. For our case, we are using google's gemma-2b instruct model. You can change the model by changing the model id.\n","We will use `AutoModelForCausalLM` for downloading and loading up the LLM model.\n","Also, we will pass on the quantization config into `AutoModelForCausalLM.from_pretrained` and do the quantization to 4bits.\n","\n","Correctly, preparing the model and tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model to teach them the different roles in a conversation. In trl we have a convenient method with setup_chat_format, which:\n","\n","Adds special tokens to the tokenizer, e.g. <|im_start|> and <|im_end|>, to indicate the start and end of a conversation.\n","Resizes the model’s embedding layer to accommodate the new tokens.\n","Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from trl import setup_chat_format\n","\n","compute_dtype = getattr(torch, \"float16\")\n","quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type='nf4',\n","        bnb_4bit_compute_dtype=compute_dtype,\n","        bnb_4bit_use_double_quant=False,\n","    )\n","model_name=\"google/gemma-2b-it\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","device_map = {\"\": 0}\n","model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quantization_config,device_map=device_map)\n","tokenizer.padding_side = 'right' # to prevent warnings\n","model, tokenizer = setup_chat_format(model, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["# Loading up LoRA config\n","\n","- In this step, we make LoRa config object to be passed on to the SFTT trainer.\n","- We are making the rank = 32.\n","- And target modules are Q,K,V for attention block and dense layers. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import LoraConfig\n"," \n","peft_config = LoraConfig(\n","        lora_alpha=32,\n","        lora_dropout=0.05,\n","        r=32,\n","        bias=\"none\",\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n","        task_type=\"CAUSAL_LM\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing Training arguments\n","\n","Pass all the necessary arguments in the TrainingArguments object, here we are not evaluating our model while training. If you want to add evaluation step, pass these arguments too.\n","-     evaluation_strategy=\"steps\",\"epoch\"\n","-     eval_steps=100,\n","-     do_eval=True,"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["output_dir = f'./peft-gemma-2b-sql-SFTT'\n","\n","args = TrainingArguments(\n","    output_dir=output_dir,              # output directory    \n","    num_train_epochs=1,                 # number of epochs to train    \n","    per_device_train_batch_size=1,      # Per device batch size to be loaded in device    \n","    gradient_accumulation_steps=4,      # Gradient accumulation steps for mini-batches   \n","    gradient_checkpointing=True,        # Gradient checkpoint    \n","    optim=\"adamw_torch_fused\",              \n","    logging_steps=25,                   # Logging steps    \n","    save_strategy=\"steps\",              # Save strategy to be steps, can also be epoch   \n","    learning_rate=2e-4,                     \n","    fp16=True,                          # fp16 to be loaded and if your gpu supports bf16 then use that    \n","    max_grad_norm=0.3,                      \n","    warmup_ratio=0.03,                      \n","    lr_scheduler_type=\"constant\",           \n","    max_steps=1000,                     # Max steps will override the training length\n","    save_steps=100,                     # Save checkpoint after every save_steps\n","    overwrite_output_dir = 'True',      # will override the dir content\n","   \n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing SFT Trainer object\n","\n","In this step we pass:\n","- Max sequence length, you can change this as per your dataset requires.\n","- We pass the base model and tokenizer\n","- Training dataset\n","- peft config\n","- packing=True, SFTTrainer supports example packing, where multiple short examples are packed in the same input sequence to increase training efficiency."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from trl import SFTTrainer\n"," \n","max_seq_length = 1024 # max sequence length for model and packing of the dataset\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=dataset['train'],\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    dataset_kwargs={\n","        \"add_special_tokens\": False,  # We template with special tokens\n","        \"append_concat_token\": False, # No need to add additional separator token\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Begin training and then saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# start training, the model will be automatically saved to the hub and the output directory\n","trainer.train()\n"," \n","# save model\n","trainer.save_model()"]},{"cell_type":"markdown","metadata":{},"source":["### Freeing up some memory"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del model\n","torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Test and evaluate the fine tuned LLM\n","\n","- We will use AutoPeftModelForCausalLm from peft library for loading up fine tuned LLM with peft adapters.\n","- Also loading the tokenizer for the fine tuned model.\n","- Using pipeline module from transformers for text-generation use case."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","from transformers import pipeline\n","peft_model = AutoPeftModelForCausalLM.from_pretrained(\n","  output_dir,\n","  device_map=\"auto\",\n","  torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(output_dir)\n","pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loading up the base model for evaluation compared to fine tuned model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["base_model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=quantization_config,device_map=device_map)\n","base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n","base_model,base_tokenizer=setup_chat_format(base_model, base_tokenizer)\n","base_model_pipe = pipeline(\"text-generation\", model=base_model, tokenizer=base_tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["# Final step of evaluation\n","\n","In this step we will use eval_dataset to evaluate our trained model and will compare it with base model too.\n","\n","- We will use apply_chat_template to prepare our prompt in conversational format and leave the assistant block empty for generation.\n","- Looping for 5 samples and generating outputs from both the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from random import randint\n","\n","eval_dataset = dataset['test']\n","for i in range(5):\n","    rand_idx = randint(0, len(eval_dataset))\n","\n","    # Test on sample\n","    prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","    base_model_prompt = base_model_pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n","\n","    outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","    base_model_outputs = base_model_pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n","\n","    print(f\"Context:\\n{eval_dataset[rand_idx]['messages'][0]['content']}\")\n","    print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n","    print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\n","    print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\\n\")\n","    print(f\"Base Model Generated Answer:\\n{base_model_outputs[0]['generated_text'][len(prompt):].strip()}\")\n","\n","    print(\"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Context:\n","You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","CREATE TABLE table_name_77 (score VARCHAR, losing_team VARCHAR, total VARCHAR)\n","Query:\n","Which Score has a Losing Team of sydney roosters, and a Total of 88?\n","Original Answer:\n","SELECT score FROM table_name_77 WHERE losing_team = \"sydney roosters\" AND total = 88\n","\n","Generated Answer:\n","SELECT score FROM table_name_77 WHERE losing_team = \"sydney roosters\" AND total = 88\n","\n","Base Model Generated Answer:\n","Which Score has a Losing Team of sydney roosters, and a Total of 88?\n","\n","\n","\n","Context:\n","You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n","SCHEMA:\n","CREATE TABLE table_name_64 (moving_to VARCHAR, transfer_fee VARCHAR, name VARCHAR)\n","Query:\n","Where is Odjidja-Ofoe, with an undisclosed transfer fee, moving to?\n","Original Answer:\n","SELECT moving_to FROM table_name_64 WHERE transfer_fee = \"undisclosed\" AND name = \"odjidja-ofoe\"\n","...\n","The\n","\n","\n","\n","Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
